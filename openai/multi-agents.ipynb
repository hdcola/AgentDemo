{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Multi-Agents SDK Demo\n",
    "\n",
    "OpenAI have released an [Agents SDK](https://openai.github.io/openai-agents-python/) and [Support MCP](https://openai.github.io/openai-agents-python/mcp/). This demo want to show how to use the SDK to make an application that call multiple agents to do a task.\n",
    "\n",
    "Install the dependencies library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai-agents==0.0.7 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stetup LLM and Agent\n",
    "\n",
    "Setup OpenAI API url, model and key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen2.5:7b'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API base URL and model with default values\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n",
    "\n",
    "# Set up OpenAI API configuration\n",
    "# Try to get API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# If not found, ask user to input it\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "OPENAI_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI, RunResultStreaming\n",
    "from agents import function_tool\n",
    "from openai.types.responses import (\n",
    "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
    "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
    "    ResponseTextDeltaEvent,  # text streaming\n",
    ")\n",
    "\n",
    "async def show_streaming_response(\n",
    "    response: RunResultStreaming,\n",
    ") -> None:\n",
    "    async for event in response.stream_events():\n",
    "        if event.type == \"raw_response_event\": \n",
    "            if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "                # this is streamed parameters for our tool call\n",
    "                print(event.data.delta, end=\"\", flush=True)\n",
    "            elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                # this is streamed final answer tokens\n",
    "                print(event.data.delta, end=\"\", flush=True)\n",
    "        elif event.type == \"agent_updated_stream_event\":\n",
    "            # this tells us which agent is currently in use\n",
    "            print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "        elif event.type == \"run_item_stream_event\":\n",
    "            # these are events containing info that we'd typically\n",
    "            # stream out to a user or some downstream process\n",
    "            if event.name == \"tool_called\":\n",
    "                # this is the collection of our _full_ tool call after our tool\n",
    "                # tokens have all been streamed\n",
    "                print()\n",
    "                print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "                print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "            elif event.name == \"tool_output\":\n",
    "                # this is the response from our tool execution\n",
    "                print(f\"> Tool Output: {event.item.raw_item['output']}\")\n",
    "\n",
    "model= OpenAIChatCompletionsModel(\n",
    "    model=OPENAI_MODEL,\n",
    "    openai_client=AsyncOpenAI(base_url=OPENAI_API_BASE,api_key=OPENAI_API_KEY),\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def add_tool(x: float, y: float) -> float:\n",
    "    \"\"\"Add X and Y to get the exact result.\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "@function_tool\n",
    "def multiply_tool(x: float, y: float) -> float:\n",
    "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
    "    answer.\"\"\"\n",
    "    return x*y\n",
    "\n",
    "math_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant\",\n",
    "    model=model,\n",
    "    tools=[add_tool,multiply_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Multiple Tools in a Conversation\n",
    "\n",
    "Our step-by-step approach will allow the LLM itself to recognize which tool should be used to process the task, so we'll start by having the Agent compute an addition and then a multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n",
      "{\"x\":1,\"y\":2}\n",
      "> Tool Called, name: add_tool\n",
      "> Tool Called, args: {\"x\":1,\"y\":2}\n",
      "> Tool Output: 3.0\n",
      "The result of 1 + 2 is 3.0."
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=math_agent,\n",
    "    input=\"What is 1 + 2?\",\n",
    ")\n",
    "\n",
    "await show_streaming_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n",
      "{\"x\":3,\"y\":5}\n",
      "> Tool Called, name: multiply_tool\n",
      "> Tool Called, args: {\"x\":3,\"y\":5}\n",
      "> Tool Output: 15.0\n",
      "The result of 3.0 * 5 is 15.0."
     ]
    }
   ],
   "source": [
    "response.to_input_list()\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=math_agent,\n",
    "    input= response.to_input_list()+[\n",
    "        {\"role\": \"user\", \"content\": \"What is last asswer * 5?\"}\n",
    "    ]\n",
    ")\n",
    "await show_streaming_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Multiple Tools in a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n",
      "{\"x\":1,\"y\":2}\n",
      "> Tool Called, name: add_tool\n",
      "> Tool Called, args: {\"x\":1,\"y\":2}\n",
      "> Tool Output: 3.0\n",
      "{\"x\":3,\"y\":5}\n",
      "> Tool Called, name: multiply_tool\n",
      "> Tool Called, args: {\"x\":3,\"y\":5}\n",
      "> Tool Output: 15.0\n",
      "The result of first adding 1 and 2, and then multiplying the result by 5 is 15.0."
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=math_agent,\n",
    "    input=\"What is the result of calculating 1+2 and then multiplying the result by 5?\",\n",
    ")\n",
    "await show_streaming_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
