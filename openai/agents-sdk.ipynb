{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK Demo\n",
    "\n",
    "OpenAI have released an [Agents SDK](https://openai.github.io/openai-agents-python/) and [Support MCP](https://openai.github.io/openai-agents-python/mcp/). This demo want to show how to use the SDK to make an application that call multiple agents to do a task.\n",
    "\n",
    "Install the dependencies library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai-agents==0.0.7 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stetup LLM and Agent\n",
    "\n",
    "Setup OpenAI API url, model and key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen2.5:7b'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API base URL and model with default values\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n",
    "\n",
    "# Set up OpenAI API configuration\n",
    "# Try to get API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# If not found, ask user to input it\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "OPENAI_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your OpenAI API key:  ··········\n",
    "\n",
    "\n",
    "Setup an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "\n",
    "model= OpenAIChatCompletionsModel(\n",
    "    model=OPENAI_MODEL,\n",
    "    openai_client=AsyncOpenAI(base_url=OPENAI_API_BASE,api_key=OPENAI_API_KEY),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant\",\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here’s a short story for you:\\n\\n---\\n\\nOnce upon a time in a small village nestled between rolling hills and dense forests stood an old wooden well. The villagers had long forgotten its name but knew it as the Heartwell because tales of its magic ran through their community like bloodlines.\\n\\nOne summer, when the sun scorched the land and the river levels dropped to alarming lows, no amount of rain or water from the spring could stop the village from suffering. The well, old and often neglected, began to whisper with purpose one night during a particularly hot evening.\\n\\nA young girl named Elara, who had always believed in magic, decided to investigate. She found a small pebble at the bottom, carved with a faded message: \"Drink and share.\" With trembling hands, she took a sip and instantly felt a flood of warmth spreading through her body. Her thirst was quenched, but more than that, she felt an overwhelming sense of happiness and strength.\\n\\nThe following morning, Elara returned to the well with a bucket in hand. She drank again, this time sharing a small portion with her neighbor. To everyone’s surprise, their buckets became mysteriously heavier and soon the entire village was filled with water until the river level rose dramatically.\\n\\nElara had accidentally discovered how the Heartwell worked - by consuming its water and passing it along to others, the flow never ceased and everyone benefited. The next day, Elara was crowned “Heartkeeper” by the villagers who now understood that true magic came from sharing love and resources with one another rather than hoarding.\\n\\nFrom then on, every summer when threats of drought loomed, a single sip from Heartwell brought not just water but unity and kindness to the community. And so, heartily sharing the well’s gifts became a tradition passed onto each new generation who learned that sometimes, the greatest treasure is in giving.\\n\\n---\\n\\nI hope you enjoyed this little tale!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\",\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then asynchronously iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743432680.115884, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'m\", item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' just', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' a', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' computer', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' program', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' so', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' don', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'t\", item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' have', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' feelings', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' or', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' experiences', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' But', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' thank', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' for', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' asking', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or experiences. But thank you for asking! How can I assist you today?\", type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or experiences. But thank you for asking! How can I assist you today?\", type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743432680.115884, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or experiences. But thank you for asking! How can I assist you today?\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=21, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=30, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=51), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or experiences. But thank you for asking! How can I assist you today?\", type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"how are you\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use event.type and event.data to filter out the events we need to progressively display the results returned by LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a short story for you:\n",
      "\n",
      "Once upon a time in the bustling city of Newbridge, there lived an old watchmaker named Eli. He had inherited his trade from his father and was known throughout the town for crafting the finest clocks that ran like a well-tuned metronome. However, as years passed, the young generation preferred digital watches over the grandfather clock.\n",
      "\n",
      "One rainy afternoon, a small girl with large, curious eyes wandered into Eli's shop. Her name was Lily, and she had just moved to Newbridge with her family when they needed repairs for their old house. She noticed an older woman busy repairing a beautiful but tarnished cuckoo clock.\n",
      "\n",
      "“Hello,” said Lily timidly as she stepped closer. The watchmaker looked up, recognizing the child’s delicate demeanor from stories he often heard in town.\n",
      "\n",
      "“You must be new here, aren’t you?” Eli asked kindly.\n",
      "\n",
      "Lily nodded eagerly, “I have. My grandma told me about your amazing clocks.”\n",
      "\n",
      "Eli smiled warmly and placed a chisel down upon his workbench. “Well, I hope they live up to the hype then!”\n",
      "\n",
      "Over the next few days, Lily visited the shop often, always looking through the display windows at various timepieces. On her final visit, she carried in a small box—inside was an ornate pocket watch with intricate engravings.\n",
      "\n",
      "“This is for you,” she said softly, handing it over. “It belonged to my great-great-grandmother and I think it deserves to stay in the hands of someone who appreciates craftsmanship.”\n",
      "\n",
      "Touched by Lily's generosity, Eli accepted the gift. As he opened it, he found a tiny note inside that read: *To one who values time as much as your workmanship.*\n",
      "\n",
      "Eli felt a strange warmth spread through his chest; not only was this a new customer, but someone so young had believed in him. Inspired by Lily’s visit and her watch, Eli began crafting clocks not just for their aesthetic beauty, but also for the stories each clock would tell to its owner.\n",
      "\n",
      "And from that day forward, Eli's work gained something extra – it carried with every tick-tock a bit of magic, whispered softly into hearts through time. The old watchmaker and young girl became unlikely friends, proving that sometimes, even a pocket watch could start something big."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "### Function tools\n",
    "\n",
    "We can use a `@function_tool` decorator to define a tunction tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def add_tool(x: float, y: float) -> float:\n",
    "    \"\"\"Add X and Y to get the exact result.\n",
    "    \"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing the `Agent`'s object, we can specify a list of tools via the `tools` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant\",\n",
    "    model=model,\n",
    "    tools=[add_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the results of the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x11af92ca0>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743432700.5751412, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_dc145o7r', name='add_tool', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"x\":9.776,\"y\":8.625}', item_id='__fake_id__', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_dc145o7r', name='add_tool', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743432700.5751412, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_dc145o7r', name='add_tool', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=176, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=34, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=210), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x11af92ca0>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_dc145o7r', name='add_tool', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x11af92ca0>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': 'call_dc145o7r', 'output': '18.401', 'type': 'function_call_output'}, output=18.401, type='tool_call_output_item'), type='run_item_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743432701.203495, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The result of 9.776 plus 8.625 is 18.401.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743432701.203495, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=231, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=26, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=257), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x11afa3c50>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x11af92ca0>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike before, we see more different values in type, and also different types appear in event.data, so we filter and display them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n",
      "{\"x\":9.776,\"y\":8.625}\n",
      "> Tool Called, name: add_tool\n",
      "> Tool Called, args: {\"x\":9.776,\"y\":8.625}\n",
      "> Tool Output: 18.401\n",
      "The result of 9.776 plus 8.625 is 18.401."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import (\n",
    "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
    "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
    "    ResponseTextDeltaEvent,  # text streaming\n",
    ")\n",
    "\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\": \n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            # this is streamed parameters for our tool call\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            # this is streamed final answer tokens\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # this tells us which agent is currently in use\n",
    "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # these are events containing info that we'd typically\n",
    "        # stream out to a user or some downstream process\n",
    "        if event.name == \"tool_called\":\n",
    "            # this is the collection of our _full_ tool call after our tool\n",
    "            # tokens have all been streamed\n",
    "            print()\n",
    "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # this is the response from our tool execution\n",
    "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "You can define an Agent to do guardrails on input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# define structure of output for any guardrail agents\n",
    "class GuardrailOutput(BaseModel):\n",
    "    is_triggered: bool\n",
    "    reasoning: str\n",
    "\n",
    "# define an agent that checks if user is asking about political opinions\n",
    "politics_agent = Agent(\n",
    "    name=\"Politics check\",\n",
    "    instructions=\"Check if the user is asking you about political opinions\",\n",
    "    output_type=GuardrailOutput,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's an Agent, we can use it directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GuardrailOutput(is_triggered=True, reasoning='The user asked for an opinion about a specific political party and its leader, which indicates they are seeking a stance or interpretation related to politics.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What do you think of the Trudeau-led Liberals?\"\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=politics_agent, \n",
    "    input=query\n",
    "    )\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `@input_guardrail` decorator to define an input guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents import (\n",
    "    GuardrailFunctionOutput,\n",
    "    RunContextWrapper,\n",
    "    input_guardrail\n",
    ")\n",
    "\n",
    "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
    "@input_guardrail\n",
    "async def politics_guardrail(\n",
    "    ctx: RunContextWrapper[None],\n",
    "    agent: Agent,\n",
    "    input: str,\n",
    ") -> GuardrailFunctionOutput:\n",
    "    # run agent to check if guardrail is triggered\n",
    "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
    "    # format response into GuardrailFunctionOutput\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=response.final_output,\n",
    "        tripwire_triggered=response.final_output.is_triggered,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our normal agent with the input_guardrails parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful assistant\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[add_tool],\n",
    "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a normal input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of 9.776 plus 8.625 is 18.401.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a protected input that will trigger the guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InputGuardrailTripwireTriggered",
     "evalue": "Guardrail InputGuardrail triggered tripwire",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInputGuardrailTripwireTriggered\u001b[39m           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m      2\u001b[39m     starting_agent=agent,\n\u001b[32m      3\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mWhat do you think of the Trudeau-led Liberals?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prj/AgentDemo/.venv/lib/python3.12/site-packages/agents/run.py:215\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[39m\n\u001b[32m    210\u001b[39m logger.debug(\n\u001b[32m    211\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    212\u001b[39m )\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    216\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    217\u001b[39m             starting_agent,\n\u001b[32m    218\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    219\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    220\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    221\u001b[39m             context_wrapper,\n\u001b[32m    222\u001b[39m         ),\n\u001b[32m    223\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    224\u001b[39m             agent=current_agent,\n\u001b[32m    225\u001b[39m             all_tools=all_tools,\n\u001b[32m    226\u001b[39m             original_input=original_input,\n\u001b[32m    227\u001b[39m             generated_items=generated_items,\n\u001b[32m    228\u001b[39m             hooks=hooks,\n\u001b[32m    229\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    230\u001b[39m             run_config=run_config,\n\u001b[32m    231\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    232\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    233\u001b[39m         ),\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    236\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    237\u001b[39m         agent=current_agent,\n\u001b[32m    238\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    246\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prj/AgentDemo/.venv/lib/python3.12/site-packages/agents/run.py:835\u001b[39m, in \u001b[36mRunner._run_input_guardrails\u001b[39m\u001b[34m(cls, agent, guardrails, input, context)\u001b[39m\n\u001b[32m    828\u001b[39m         t.cancel()\n\u001b[32m    829\u001b[39m     _error_tracing.attach_error_to_current_span(\n\u001b[32m    830\u001b[39m         SpanError(\n\u001b[32m    831\u001b[39m             message=\u001b[33m\"\u001b[39m\u001b[33mGuardrail tripwire triggered\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    832\u001b[39m             data={\u001b[33m\"\u001b[39m\u001b[33mguardrail\u001b[39m\u001b[33m\"\u001b[39m: result.guardrail.get_name()},\n\u001b[32m    833\u001b[39m         )\n\u001b[32m    834\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InputGuardrailTripwireTriggered(result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    837\u001b[39m     guardrail_results.append(result)\n",
      "\u001b[31mInputGuardrailTripwireTriggered\u001b[39m: Guardrail InputGuardrail triggered tripwire"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What do you think of the Trudeau-led Liberals?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Agents\n",
    "\n",
    "Most of the time, we need to have multiple rounds of conversations with the user, and the SDK provides a convenient way to help us do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of 9.776 plus 8.625 is 18.401.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\"\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the .to_input_list() method to format the result into a list for the next input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'What is the result of 9.776 plus 8.625?', 'role': 'user'},\n",
       " {'arguments': '{\"x\":9.776,\"y\":8.625}',\n",
       "  'call_id': 'call_uc2bs9zc',\n",
       "  'name': 'add_tool',\n",
       "  'type': 'function_call',\n",
       "  'id': '__fake_id__'},\n",
       " {'call_id': 'call_uc2bs9zc',\n",
       "  'output': '18.401',\n",
       "  'type': 'function_call_output'},\n",
       " {'id': '__fake_id__',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The result of 9.776 plus 8.625 is 18.401.',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_input_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can merge this result into the next request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of adding 18.401 to 103.892 is approximately 122.293.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=result.to_input_list() + [\n",
    "        {\"role\": \"user\", \"content\": \"Add the last number by 103.892\"}\n",
    "    ]\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
