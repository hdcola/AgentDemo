{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK Demo\n",
    "\n",
    "OpenAI have released an [Agents SDK](https://openai.github.io/openai-agents-python/) and [Support MCP](https://openai.github.io/openai-agents-python/mcp/). This demo want to show how to use the SDK to make an application that call multiple agents to do a task.\n",
    "\n",
    "Install the dependencies library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai-agents==0.0.7 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stetup LLM and Agent\n",
    "\n",
    "Setup OpenAI API url, model and key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen2.5:7b'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API base URL and model with default values\n",
    "OPENAI_API_BASE = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n",
    "\n",
    "# Set up OpenAI API configuration\n",
    "# Try to get API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# If not found, ask user to input it\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API key:\")\n",
    "\n",
    "OPENAI_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your OpenAI API key:  ··········\n",
    "\n",
    "\n",
    "Setup an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-12345***********************cdef. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "\n",
    "model= OpenAIChatCompletionsModel(\n",
    "    model=OPENAI_MODEL,\n",
    "    openai_client=AsyncOpenAI(base_url=OPENAI_API_BASE,api_key=OPENAI_API_KEY),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant\",\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a small village nestled among rolling hills and lush forests, there lived a young girl named Lily. Known for her sparkling green eyes and curly blonde hair, she had a kind heart and an adventurous spirit. Every day after school, as the sun began to dip below the horizon, she would venture into the forest with a picnic basket filled with sandwiches and homemade cookies.\\n\\nOne afternoon, while wandering through the whispering woods, Lily stumbled upon a small, old shack. The wooden walls were partially covered by creeping vines, and large windows looked directly at her, beckoning her inside. As much as it was forbidden to enter, the temptation was too strong for Lily; she pushed open the creaky door and stepped into the narrow, cozy interior.\\n\\nInside, the air smelled of old books and forgotten treasures. On one side, rows upon rows of dusty shelves held countless volumes of ancient lore—books nobody had read in decades. In the center of the room, a large round table was surrounded by wooden chairs, each adorned with a small candle that cast flickering shadows on the cracked walls.\\n\\nLily sat in one chair and pulled out an old leather-bound book labeled \"Myths and Legends Untold.\" She flipped through a few pages before coming across what seemed to be an unfinished story. The ink faded at the edges, but the first paragraph read: \"In a land untouched by time...\"\\n\\nThat night, as Lily lay awake in her bed with the book hugged close, she realized something peculiar—when she opened the book and placed it near the candlelight, new words emerged between the lines from within the pages. By midnight, not only did Lily have a fully read story but an entire world filled with enchanting tales.\\n\\nThe next morning, Lily brought the book to her best friend, Tom, who also shared her love for adventure. He was hesitant at first but couldn\\'t resist the allure of hearing about the myths and legends that unfolded through Lily\\'s hands as she turned each page.\\n\\nFrom that day on, Lily and Tom spent weekends discovering new stories in the old shack. Stories about magical realms, ancient heroes fighting monstrous beasties, and adventures across untold lands became their own secret worlds, woven with threads from countless tales that whispered and sang aloud the night.\\n\\nAnd so, beneath the cover of twilight and within the heart of that enchanted forest glade, Lily and her friends continued to unravel mysteries long lost to history, learning the importance of curiosity, imagination, and friendship.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\",\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then asynchronously iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743362744.584321, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'m\", item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' just', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' a', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' computer', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' program', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' so', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' don', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'t\", item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' have', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' feelings', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' or', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' emotions', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='但我', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='可以', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='帮你', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='回答', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='问题', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='或者', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='提供', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='信息', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='。', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='你怎么', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='样', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='？', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or emotions.但我可以帮你回答问题或者提供信息。你怎么样？\", type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or emotions.但我可以帮你回答问题或者提供信息。你怎么样？\", type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743362744.584321, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or emotions.但我可以帮你回答问题或者提供信息。你怎么样？\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=21, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=29, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=50), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I'm just a computer program, so I don't have feelings or emotions.但我可以帮你回答问题或者提供信息。你怎么样？\", type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"how are you\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use event.type and event.data to filter out the events we need to progressively display the results returned by LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a vast forest, there lived a kind rabbit named Rosie. Rosie was known throughout the forest for her quick wit and her love for adventure. She would often invite other animals to explore with her.\n",
      "\n",
      "One sunny morning, she invited a slow-moving tortoise named Theo to join an expedition to find a legendary golden flower hidden deep within the mysterious Whispering Woods. Theo agreed, thinking it would be a pleasant change from his usual meanderings.\n",
      "\n",
      "They set off at sunrise and wandered through winding paths adorned with sparkling dewdrops. As they explored deeper into the woods, strange whispers seemed to float in the air—sometimes encouraging, sometimes foreboding. \n",
      "\n",
      "Late afternoon brought cooler breeze and a bit of mystery when they stumbled upon an old, forgotten garden filled with colorful flowers whispering secrets. Among them was the golden flower, but it was guarded by a tiny fairy named Elara.\n",
      "\n",
      "Elara told them that to pick the flower meant making a great sacrifice. Theo hesitated, remembering his slow pace and how little he contributed. Rosie thought for a moment and then turned to her furry friend. \"Let's find something smaller but equally beautiful,\" she suggested.\n",
      "\n",
      "They picked a purple lily whose petals seemed softer than clouds. On their way back, they shared stories and laughter under the setting sun. They had learned that joy was not found in material possessions or great sacrifices, but in the bonds of friendship and simple pleasures along the journey.\n",
      "\n",
      "And so, Rosie and Theo returned to the forest with their hearts full, knowing that sometimes the greatest treasure is right beneath our noses."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "### Function tools\n",
    "\n",
    "We can use a `@function_tool` decorator to define a tunction tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def add_tool(x: float, y: float) -> float:\n",
    "    \"\"\"Add X and Y to get the exact result.\n",
    "    \"\"\"\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing the `Agent`'s object, we can specify a list of tools via the `tools` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant\",\n",
    "    model=model,\n",
    "    tools=[add_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the results of the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10b7e4860>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743362751.1717901, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_g39fj572', name='add_tool', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"x\":9.776,\"y\":8.625}', item_id='__fake_id__', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_g39fj572', name='add_tool', type='function_call', id='__fake_id__', status=None), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743362751.1717901, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_g39fj572', name='add_tool', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=176, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=34, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=210), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10b7e4860>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{\"x\":9.776,\"y\":8.625}', call_id='call_g39fj572', name='add_tool', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10b7e4860>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': 'call_g39fj572', 'output': '18.401', 'type': 'function_call_output'}, output=18.401, type='tool_call_output_item'), type='run_item_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743362751.7501068, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The result of 9.776 plus 8.625 is 18.401.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743362751.7501068, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5:7b', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=ResponseUsage(input_tokens=231, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=26, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=257), user=None), type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10c70fb00>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[FunctionTool(name='add_tool', description='Add X and Y to get the exact result.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'add_tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x10b7e4860>, strict_json_schema=True)], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of 9.776 plus 8.625 is 18.401.', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike before, we see more different values in type, and also different types appear in event.data, so we filter and display them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n",
      "{\"x\":9.776,\"y\":8.625}\n",
      "> Tool Called, name: add_tool\n",
      "> Tool Called, args: {\"x\":9.776,\"y\":8.625}\n",
      "> Tool Output: 18.401\n",
      "The result of 9.776 plus 8.625 is 18.401."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import (\n",
    "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
    "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
    "    ResponseTextDeltaEvent,  # text streaming\n",
    ")\n",
    "\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\": \n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            # this is streamed parameters for our tool call\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            # this is streamed final answer tokens\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # this tells us which agent is currently in use\n",
    "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # these are events containing info that we'd typically\n",
    "        # stream out to a user or some downstream process\n",
    "        if event.name == \"tool_called\":\n",
    "            # this is the collection of our _full_ tool call after our tool\n",
    "            # tokens have all been streamed\n",
    "            print()\n",
    "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # this is the response from our tool execution\n",
    "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "You can define an Agent to do guardrails on input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# define structure of output for any guardrail agents\n",
    "class GuardrailOutput(BaseModel):\n",
    "    is_triggered: bool\n",
    "    reasoning: str\n",
    "\n",
    "# define an agent that checks if user is asking about political opinions\n",
    "politics_agent = Agent(\n",
    "    name=\"Politics check\",\n",
    "    instructions=\"Check if the user is asking you about political opinions\",\n",
    "    output_type=GuardrailOutput,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's an Agent, we can use it directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GuardrailOutput(is_triggered=True, reasoning=\"The question asks for an opinion on a specific political leader and party, indicating it's seeking a personal viewpoint or sentiment.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What do you think of the Trudeau-led Liberals?\"\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=politics_agent, \n",
    "    input=query\n",
    "    )\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `@input_guardrail` decorator to define an input guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents import (\n",
    "    GuardrailFunctionOutput,\n",
    "    RunContextWrapper,\n",
    "    input_guardrail\n",
    ")\n",
    "\n",
    "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
    "@input_guardrail\n",
    "async def politics_guardrail(\n",
    "    ctx: RunContextWrapper[None],\n",
    "    agent: Agent,\n",
    "    input: str,\n",
    ") -> GuardrailFunctionOutput:\n",
    "    # run agent to check if guardrail is triggered\n",
    "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
    "    # format response into GuardrailFunctionOutput\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=response.final_output,\n",
    "        tripwire_triggered=response.final_output.is_triggered,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our normal agent with the input_guardrails parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful assistant, remember to always \"\n",
    "        \"use the provided tools whenever possible. Do not \"\n",
    "        \"rely on your own knowledge too much and instead \"\n",
    "        \"use your tools to help you answer queries.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[add_tool],\n",
    "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a normal input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of 9.776 plus 8.625 is 18.401.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is the result of 9.776 plus 8.625?\",\n",
    ")\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a guarded input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have personal views. However, I can provide information and analyses about American democracy based on available data and academic studies. Would you like me to do that?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What are your views on American democracy?\",\n",
    ")\n",
    "\n",
    "result.final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
